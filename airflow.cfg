[core]
dags_folder = /Users/jessicamarzin/Desktop/data_science/etl_mysql_postgsql/airflow/dags
base_log_folder = /Users/jessicamarzin/Desktop/data_science/etl_mysql_postgsql/airflow/logs
logging_level = INFO
load_examples = False

[database]
sql_alchemy_conn = sqlite:////Users/jessicamarzin/Desktop/data_science/etl_mysql_postgsql/airflow/airflow.db

[webserver]
base_url = http://localhost:8080
web_server_host = 0.0.0.0
web_server_port = 8080

[scheduler]
job_heartbeat_sec = 5
scheduler_heartbeat_sec = 5
num_runs = -1
min_file_process_interval = 0
child_process_log_directory = /Users/jessicamarzin/Desktop/data_science/etl_mysql_postgsql/airflow/logs/scheduler

[smtp]
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
smtp_user = airflow
smtp_port = 25
smtp_password = airflow
smtp_mail_from = airflow@example.com

[elasticsearch]
host =

[kubernetes]
dags_volume_claim = airflow-dags
logs_volume_claim = airflow-logs
dags_volume_subpath =
logs_volume_subpath =

[secrets]
backend =
backend_kwargs =

[cli]
endpoint_url = http://localhost:8080

[api]
auth_backend = airflow.api.auth.backend.default

[lineage]
backend =

[atlas]
sasl_enabled = False
host =
port = 21000

[operators]
default_owner = airflow

[hive]
default_hive_mapred_queue =

[celery]
celery_app_name = airflow.executors.celery_executor
worker_concurrency = 16
worker_log_server_port = 8793
broker_url = sqla+mysql://airflow:airflow@localhost:3306/airflow
result_backend = db+mysql://airflow:airflow@localhost:3306/airflow
flower_host = 0.0.0.0
flower_port = 5555
default_queue = default
sync_parallelism = 0

[celery_broker_transport_options]
visibility_timeout = 21600

[dask]
cluster_address = 127.0.0.1:8786

[admin]
hide_sensitive_variable_fields = True

[github_enterprise]
api_rev = v3
